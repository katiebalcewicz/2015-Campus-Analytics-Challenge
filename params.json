{"name":"Wells Fargo 2015 Campus Analytics Challenge","tagline":"","body":"For the 2015 Wells Fargo Campus Analytics Challenge, students were asked to analyze a set of 220,377 anonymized social media posts to gain new insights into how customers discuss banking topics. More information about the challenge can be found on [MindSumo](https://www.mindsumo.com/contests/wells-fargo). For our solution, we used the R programming language to explore the data, classify the posts into categories, and analyze the sentiment of each post. The code and the other materials used in the solution can be found at [github.com/katiebalcewicz/2015-Campus-Analytics-Challenge](https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge).  \r\n\r\n#Structuring the Data\r\nThe raw data that we were given was in a pipe-delimited text file that was easily read into a data frame. \r\n\r\n       #Use the setwd() command to set the working directory to the folder that contains the dataset\r\n       setwd(\"~/R/Wells Fargo Competition\")\r\n       #Create a Data Frame from the text file\r\n       df = read.table('dataset.txt',sep=\"|\",header=T)\r\n\r\nThe first five columns were clean and easy to analyze. The sixth column held the anonymized text of the social media post, which was very messy.\r\n\r\n![Initial Data Frame](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/initial%20data%20frame.png)\r\n\r\nIn order to convert the data into a usable, structured format, we added columns to the data frame containing cleaned text, bank mentions, and sentiment scores.\r\n\r\n##Cleaning the Data\r\nThe first step in structuring the data was cleaning it so that it could be analyzed. \r\n\r\n###Removing non-ASCII Characters\r\nSome functions in the R packages we used did not like non-ASCII characters. The first step was to remove them from `FullText`. We defined a function called `removeOffendingCharacters` that scans through each post and replaces non-ASCII characters with \"\". Calling this function removes all non-ASCII characters from the `FullText` column.\r\n\r\n       removeOffendingCharacters = function(df)\r\n       {\r\n         df.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n         colnames(df.texts.clean) = 'FullText'\r\n         df$FullText = df.texts.clean$FullText\r\n         return(df)\r\n       }\r\n\r\n###Cleaning the Text\r\nWe made several other modifications to the text so that it was easier for us to analyze. These include:\r\n* Stripping extra whitespace\r\n* Converting all letters to lowercase\r\n* Removing stopwords (filler words that do not contribute to the essential meaning of the sentence, such as \"she\", \"this\", and \"with\".\r\n*Removing numbers\r\n*Removing all punctuation except '_'\r\n\r\nWe again defined a function that makes these modifications. This function converts the `FullText` column into a corpus and applies several of the functions from the tm package to clean the text before converting it back into a data frame. This is then added to the main data frame in a new column called `FullTextClean`.\r\n\r\n       cleanText = function(df)\r\n       {\r\n         require(tm)\r\n         docs = Corpus(VectorSource(df$FullText))\r\n         docs = tm_map(docs, stripWhitespace)\r\n         docs = tm_map(docs, tolower) \r\n         docs = tm_map(docs, removeWords, stopwords(\"english\"))  \r\n         docs = tm_map(docs, removeNumbers)   \r\n         docs = tm_map(docs, PlainTextDocument)\r\n         removeSomePunct = function(x) gsub(\"[^[:alnum:][:blank:]_]\", \"\", x)\r\n         docs = tm_map(docs, content_transformer(removeSomePunct))\r\n         docs = tm_map(docs, stripWhitespace)  \r\n         docs = tm_map(docs, PlainTextDocument)\r\n                \r\n         df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n         colnames(df.texts.cleaner) = 'FullText'\r\n         df$FullTextClean = df.texts.cleaner$FullText\r\n         return(df)\r\n       }\r\n\r\n###Stemming the words\r\n\"Stemming\" is a process by which all words that have the same stem (such as \"analyze\", \"analyzed\", and \"analyzing\") are all converted to the same stem (\"analyz\"). This allows us to treat each of these words as the same when we analyze the data. We defined a function that applies this process to the `FullTextClean` column and adds the results to the main data frame in a new column called `FullTextStemmed`.\r\n\r\n       stemText = function(df)\r\n       {\r\n         require(tm)\r\n         require(SnowballC)\r\n         docs = Corpus(VectorSource(df$FullTextClean))\r\n         docs = tm_map(docs, stemDocument)\r\n         docs = tm_map(docs, PlainTextDocument)\r\n         df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n         colnames(df.texts.cleaner) = 'FullText'\r\n         df$FullTextStemmed = df.texts.cleaner$FullText\r\n         return(df)\r\n       }\r\n\r\n###Stem Completion\r\nStem Completion converts the stems from the previous step (\"analyz\") back into a dictionary word (\"analyze\"). Unfortunately, this algorithm was too computationally intensive to apply to the full dataset. We designed this function to use multithreading in the hopes that we might get the chance to run the code on a computer with more cores. If you were to run this function, it would add another column to the main data frame called `FullTextCompleted`.\r\n\r\n       stemCompleteText = function(df)\r\n       {\r\n         require(plyr)\r\n         require(doMC)\r\n         require(tm)\r\n         doMC::registerDoMC(cores=3) #Change the number of threads here to match your machine.\r\n         docs = Corpus(VectorSource(df$FullTextStemmed))\r\n         dictionary = Corpus(VectorSource(df$FullText))\r\n                \r\n         stemCompletion2 <- function(y, dictionary) \r\n         {\r\n           y = unlist(strsplit(as.character(y), \" \"))\r\n           y = y[y != \"\"]\r\n           y = stemCompletion(y, dictionary=dictionary)\r\n           y = paste(y, sep=\"\", collapse=\" \")\r\n           stripWhitespace(y)\r\n         }\r\n                \r\n         completedText = llply(docs, stemCompletion2, dictionary=dictionary, .parallel = TRUE)\r\n         df$FullTextCompleted = unlist(completedText)\r\n         return(df)\r\n       }\r\n\r\n\r\n##Analyzing bank mentions\r\nWe identified which bank was mentioned in each post. We also counted the number of banks mentioned in each post. This function adds six new columns to the data frame. The first 5, for banks A-E, hold a 1 if the bank is mentioned and a 0 if it is not. The sixth is a sum of the previous five columns and is equivalent to the number of banks mentioned in the post. Most posts mention one bank.\r\n\r\n       bankMentions = function(df)\r\n       {\r\n         df$BankA = 0\r\n         df$BankB = 0\r\n         df$BankC = 0\r\n         df$BankD = 0\r\n         df$BankE = 0\r\n         df$NumBanks = 0\r\n         for( i in 1:nrow(df))\r\n         {\r\n           if (grepl(\"BankA\",df$FullText[i]))  df$BankA[i] = 1\r\n           if (grepl(\"BankB\",df$FullText[i]))  df$BankB[i] = 1\r\n           if (grepl(\"BankC\",df$FullText[i]))  df$BankC[i] = 1\r\n           if (grepl(\"BankD\",df$FullText[i]))  df$BankD[i] = 1\r\n           if (grepl(\"banke\",df$FullText[i]))  df$BankE[i] = 1\r\n           df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]\r\n         }\r\n         return(df)\r\n       }\r\n\r\n##Sentiment Analysis\r\nsentiment score vs sentiment density\r\n\r\n#Exploratory Data Analysis\r\nDocument-term matrix, word frequencies, clustering\r\n\r\n##Plot a Bar Graph\r\n![Bar Graph of Frequent Words](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/bar%20chart.png)\r\n\r\n##Create a Word Cloud\r\n![Word Cloud of frequent words](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/word%20cloud%2002.png)\r\n\r\n##Find Words Associated with Each Bank\r\n![Words highly associated with each bank](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/assocs%202.png)\r\n\r\n##Plot a Cluster Dendrogram\r\n\r\n\r\n#Classify the Posts\r\n![Important fields for classification](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/classifier%20importance%20plot.png)\r\n\r\n\r\n#Results\r\n\r\n##Boxplot of Sentiment Density Scores By Topic\r\n![Boxplot of Sentiment Density Scores by Topic](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/boxplot.png)\r\n\r\n##Average Sentiment Density Scores by Topic\r\n![Bar Graph of Average Sentiment Density Scores by Topic](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/Average%20sentiment%20density%20score.png)\r\n\r\n##Average Sentiment Density Scores by Topic and Bank\r\n![Boxplot of Average Sentiment Density Score by Topic and Bank](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/Average%20score%20by%20topic%20and%20bank.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}