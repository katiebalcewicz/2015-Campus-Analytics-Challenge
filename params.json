{"name":"Wells Fargo 2015 Campus Analytics Challenge","tagline":"","body":"For the 2015 Wells Fargo Campus Analytics Challenge, students were asked to analyze a set of 220,377 anonymized social media posts to gain new insights into how customers discuss banking topics. More information about the challenge can be found on [MindSumo](https://www.mindsumo.com/contests/wells-fargo). For our solution, we used the R programming language to explore the data, classify the posts into categories, and analyze the sentiment of each post. The code and the other materials used in the solution can be found at [github.com/katiebalcewicz/2015-Campus-Analytics-Challenge](https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge).  \r\n\r\n#Structuring the Data\r\nThe raw data that we were given was in a pipe-delimited text file that was easily read into a data frame. \r\n\r\n    #Use the setwd() command to set the working directory to the folder that contains the dataset\r\n    setwd(\"~/R/Wells Fargo Competition\")\r\n    #Create a Data Frame from the text file\r\n    df = read.table('dataset.txt',sep=\"|\",header=T)\r\n\r\nThe first five columns were clean and easy to analyze. The sixth column held the anonymized text of the social media post, which was very messy.\r\n\r\n![Initial Data Frame](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/initial%20data%20frame.png)\r\n\r\nIn order to convert the data into a usable, structured format, we added columns to the data frame containing cleaned text, bank mentions, and sentiment scores.\r\n\r\n##Cleaning the Text\r\nThe first step in structuring the data was cleaning the raw text so that it could be more easily analyzed.\r\n\r\n###Removing non-ASCII Characters\r\nSome functions in the R packages we used did not like non-ASCII characters. The first step was to remove them from `FullText`. We defined a function called `removeOffendingCharacters` that scans through each post and removes non-ASCII characters from the `FullText` column.\r\n\r\n    removeOffendingCharacters = function(df)\r\n    {\r\n      df.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n      colnames(df.texts.clean) = 'FullText'\r\n      df$FullText = df.texts.clean$FullText\r\n      return(df)\r\n    }\r\n\r\n###Cleaning the Text\r\nWe made several other modifications to the text so that it was easier for us to analyze. These include:\r\n* Stripping extra whitespace\r\n* Converting all letters to lowercase\r\n* Removing stopwords (filler words that do not contribute to the essential meaning of the sentence, such as \"she\", \"this\", and \"with\".\r\n* Removing numbers\r\n* Removing all punctuation except '_'\r\n\r\nWe again defined a function that makes these modifications. This function converts the `FullText` column into a corpus and applies several of the functions from the tm package to clean the text before converting it back into a data frame. This is then added to the main data frame in a new column called `FullTextClean`.\r\n\r\n    cleanText = function(df)\r\n    {\r\n      require(tm)\r\n      docs = Corpus(VectorSource(df$FullText))\r\n      docs = tm_map(docs, stripWhitespace)\r\n      docs = tm_map(docs, tolower) \r\n      docs = tm_map(docs, removeWords, stopwords(\"english\"))  \r\n      docs = tm_map(docs, removeNumbers)   \r\n      docs = tm_map(docs, PlainTextDocument)\r\n      removeSomePunct = function(x) gsub(\"[^[:alnum:][:blank:]_]\", \"\", x)\r\n      docs = tm_map(docs, content_transformer(removeSomePunct))\r\n      docs = tm_map(docs, stripWhitespace)  \r\n      docs = tm_map(docs, PlainTextDocument)\r\n          \r\n      df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n      colnames(df.texts.cleaner) = 'FullText'\r\n      df$FullTextClean = df.texts.cleaner$FullText\r\n      return(df)\r\n    }\r\n\r\n###Stemming the words\r\n\"Stemming\" is a process by which all words that have the same stem (such as \"analyze\", \"analyzed\", and \"analyzing\") are all converted to the same stem (\"analyz\"). This allows us to treat each of these words as the same when we analyze the data. We defined a function that applies this process to the `FullTextClean` column and adds the results to the main data frame in a new column called `FullTextStemmed`.\r\n\r\n    stemText = function(df)\r\n    {\r\n      require(tm)\r\n      require(SnowballC)\r\n      docs = Corpus(VectorSource(df$FullTextClean))\r\n      docs = tm_map(docs, stemDocument)\r\n      docs = tm_map(docs, PlainTextDocument)\r\n      df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n      colnames(df.texts.cleaner) = 'FullText'\r\n      df$FullTextStemmed = df.texts.cleaner$FullText\r\n      return(df)\r\n    }\r\n\r\n###Stem Completion\r\nStem Completion converts the stems from the previous step (\"analyz\") back into a dictionary word (\"analyze\"). Unfortunately, this algorithm was too computationally intensive to apply to the full dataset. We designed this function to use multithreading in the hopes that we might get the chance to run the code on a computer with more cores. If you were to run this function, it would add another column to the main data frame called `FullTextCompleted`.\r\n\r\n    stemCompleteText = function(df)\r\n    {\r\n      require(plyr)\r\n      require(doMC)\r\n      require(tm)\r\n      doMC::registerDoMC(cores=3) #Change the number of threads here to match your machine.\r\n      docs = Corpus(VectorSource(df$FullTextStemmed))\r\n      dictionary = Corpus(VectorSource(df$FullText))\r\n             \r\n      stemCompletion2 <- function(y, dictionary) \r\n      {\r\n        y = unlist(strsplit(as.character(y), \" \"))\r\n        y = y[y != \"\"]\r\n        y = stemCompletion(y, dictionary=dictionary)\r\n        y = paste(y, sep=\"\", collapse=\" \")\r\n        stripWhitespace(y)\r\n      }\r\n             \r\n      completedText = llply(docs, stemCompletion2, dictionary=dictionary, .parallel = TRUE)\r\n      df$FullTextCompleted = unlist(completedText)\r\n      return(df)\r\n    }\r\n\r\n\r\n##Analyzing bank mentions\r\nThe next step to structuring the data was separating it by bank. We identified which bank was mentioned in each post. We also counted the number of banks mentioned in each post. This function adds six new columns to the data frame. The first 5, for banks A-E, hold a 1 if the bank is mentioned and a 0 if it is not. The sixth is a sum of the previous five columns and is equivalent to the number of banks mentioned in the post. Most posts mention one bank.\r\n\r\n    bankMentions = function(df)\r\n    {\r\n      df$BankA = 0\r\n      df$BankB = 0\r\n      df$BankC = 0\r\n      df$BankD = 0\r\n      df$BankE = 0\r\n      df$NumBanks = 0\r\n      for( i in 1:nrow(df))\r\n      {\r\n        if (grepl(\"BankA\",df$FullText[i]))  df$BankA[i] = 1\r\n        if (grepl(\"BankB\",df$FullText[i]))  df$BankB[i] = 1\r\n        if (grepl(\"BankC\",df$FullText[i]))  df$BankC[i] = 1\r\n        if (grepl(\"BankD\",df$FullText[i]))  df$BankD[i] = 1\r\n        if (grepl(\"banke\",df$FullText[i]))  df$BankE[i] = 1\r\n        df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]\r\n      }\r\n      return(df)\r\n    }\r\n\r\n##Sentiment Analysis\r\nOur third step for structuring the data was to determine whether each post was positive or negative. We assigned each post a score based on the number of positive and negative words that it contained. We began by importing lists of positive words and negative words from text files. Then, for each post, we counted the number of positive word matches and negative words matches. The number of positive matches minus the number of negative matches equals the sentiment score. This is added to the data frame in a column called `SentimentScore`. However, we found that this was skewed towards extremes for posts with more words (i.e. facebook posts). In order to normalize the sentiment score, we created an additional column called `SentimentDensityScore` that was calculated by dividing the sentiment score by the number of words in the post. We also added two columns called `very.pos` and `very.neg` which contain 1 if the sentiment score is greater than 2 or less than -2, respectively, 0 otherwise. The function below applies the above procedure using the `plyr` package.\r\n\r\n    sentimentAnalysis = function(df)\r\n    {\r\n      pos <- scan('positive-words.txt',what='character',comment.char=';')\r\n      neg <- scan('negative-words.txt',what='character',comment.char=';')\r\n      require(plyr)\r\n      require(stringr)\r\n      \r\n      scores = laply(df$FullTextClean, function(sentence, pos.words, neg.words) {\r\n        sentence = as.character(sentence)\r\n        word.list = str_split(sentence, '\\\\s+')\r\n        words = unlist(word.list)\r\n        words.length = length(words)\r\n    \r\n        pos.matches = match(words, pos.words)\r\n        neg.matches = match(words, neg.words)\r\n        \r\n        pos.matches = !is.na(pos.matches)\r\n        neg.matches = !is.na(neg.matches)\r\n    \r\n        score = sum(pos.matches) - sum(neg.matches)\r\n        if(words.length == 0) score.density = 0\r\n        else score.density = score/words.length\r\n        score.list = c(score, score.density)\r\n        \r\n        return(score.list)\r\n      }, pos, neg, .progress = 'text')\r\n      \r\n      scores.df = data.frame(SentimentScore=scores[,1], SentimentDensity=scores[,2])\r\n      df$SentimentScore = scores[,1]\r\n      df$SentimentDensity = scores[,2]\r\n      df$very.pos = as.numeric(df$SentimentScore >= 2)\r\n      df$very.neg = as.numeric(df$SentimentDensity <= -2)\r\n      return(df)\r\n    }\r\n\r\n#Exploratory Data Analysis\r\nOnce the data was cleaned up, we were able to begin our initial exploratory analysis. We began by creating a Document Term Matrix, which contained 95109 terms. Removing the terms with greater than 99.9% sparsity left us with a much more manageable matrix of only 1323 terms. Since the matrix was so large (220377 rows), we converted it into a Simple Triplet Matrix so that we could perform computations on it.\r\n\r\n    docs = Corpus(VectorSource(df$FullTextStemmed))\r\n    dtm = DocumentTermMatrix(docs)   \r\n    dtm.sparse = dtms <- removeSparseTerms(dtm, 0.999)\r\n    dtm.sparse.simple = as.simple_triplet_matrix(dtm.sparse) \r\n\r\nWe computed the word frequencies by summing the entries of each column and sorting the list from most to least occurrences. We removed the first nine most frequent words, which included the name of each bank and the words that were used by the competition organizers to anonymize the posts (\"name\", \"internet\", \"twit_hndl\", and \"name_resp\"), because they occurred much more often than any other words.\r\n\r\n    freq <- sort(col_sums(dtm.sparse.simple), decreasing=TRUE)   \r\n    freq = freq[-c(1:9)]\r\n    wf <- data.frame(word=names(freq), freq=freq)  \r\n\r\n##Plot a Bar Graph\r\nWe created a bar graph of the words occurring more than 6000 times.\r\n![Bar Graph of Frequent Words](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/bar%20chart.png)\r\n\r\n##Create a Word Cloud\r\nWe created a word cloud of the 60 most frequent words.\r\n![Word Cloud of frequent words](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/word%20cloud%2002.png)\r\n\r\n##Find Words Associated with Each Bank\r\nUsing the `findAssocs` function, we identified words that are highly correlated with the mention of each bank name. Notice that BankA and BankB are both frequently mentioned in association with buildings and sporting events that they sponsor. This easily allows us to identify the name of the banks. For example, try googling \"center arena Philadelphia\". \r\n![Words highly associated with each bank](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/assocs%202.png)\r\n\r\n\r\n#Classify the Posts\r\nBased on the frequent words found in our exploratory data analysis, we manually identified four topics: Customer Service, Bank Services, Public Relations, and Nonsense. We pulled a random subset of 600 posts and manually labeled each one with the topic we identified. The following code demonstrates how we developed a Random Forest Classifier that we used to label the rest of the posts in the dataset.\r\n\r\n##Import the training dataset\r\nWe imported the training data set that was manually labeled by topic. We applied the same cleaning and structuring that we used for the main dataset. We then created a non-sparse Document Term Matrix to identify which posts held which words. We converted this back to a data frame so that we could use the classifier on it. We added three columns to the data frame that were also used in the classification: MediaType, very.pos, and very.neg. Finally we added the topic so that the classifier could be trained to identify it.\r\n\r\n    train = read.csv(\"train.csv\")\r\n    train = createDataFrame(train)\r\n    docs.train = Corpus(VectorSource(train$FullTextStemmed))\r\n    dtm.train = DocumentTermMatrix(docs.train) \r\n    dtm.sparse.train = removeSparseTerms(dtm.train, 0.99)\r\n    terms = as.data.frame(as.matrix(dtm.sparse.train))\r\n    terms$MediaType = train$MediaType\r\n    terms$very.pos = train$very.pos\r\n    terms$very.neg = train$very.neg\r\n    terms$Topic = train$Topic\r\n\r\n##Split the training dataset for cross validation\r\nWe used the holdout method of cross-validation to estimate the accuracy of our classifier. Our holdout set accounted for about 1/3 of the data. Using a Random Forest classifier, the accuracy comes out to about 78%. We also tested a CART decision tree created using rpart and a Naive Bayes classifier, but both had worse accuracy.\r\n\r\n    terms.train = droplevels(terms[c(1:399),])\r\n    terms.test = droplevels(terms[c(400:nrow(terms)),])\r\n    classifier = randomForest(Topic ~ ., data = terms.train)\r\n    prediction = predict(classifier, terms.test)\r\n    terms.test$Prediction = prediction\r\n    terms.test$Accuracy = as.numeric(terms.test$Topic == terms.test$Prediction)\r\n    accuracy = sum(terms.test$Accuracy)/nrow(terms.test)\r\n\r\n##Build the Random Forest Model\r\nA random forest classifier is essentially a set of many decision trees whose results are averaged to decide the classification. In this case, our random forest has 500 trees.\r\n\r\n    classifier = randomForest(Topic ~ ., data = terms)\r\n\r\nAlthough the classifier cannot be easily visualized, since it has so many trees, we can view which fields were important in the classification.\r\n\r\n![Important fields for classification](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/classifier%20importance.png)\r\n\r\n##Create the test data frame for the classifier\r\nTo create the testing data frame, we used the same approach as for creating the training data frame. We created a non-sparse Document Term Matrix, converted back to a data frame, and added the additional fields of MediaType, very.pos, and very.neg.\r\n\r\n    test.terms = as.data.frame(as.matrix(dtm.sparse))\r\n    test.terms$MediaType = df$MediaType\r\n    test.terms$very.pos = df$very.pos\r\n    test.terms$very.neg = df$very.neg\r\n\r\n##Apply the classifier to the test data frame\r\nWe applied the classifier to the test data frame and put the results in the main data frame in a column called `Topic`.\r\n   \r\n    prediction = predict(classifier, test.terms)\r\n    df$Topic = prediction\r\n\r\n\r\n#Results\r\nThe final data frame has 13 columns in addition to the original 6. Based on these columns, we were able to create several graphs from which we drew conclusions about the data.\r\n\r\n##Boxplot of Sentiment Density Scores By Topic\r\nWe created a boxplot that plotted Sentiment Density Scores by topic. From this you can see that nonsense posts made up the majority of all posts and have the greatest spread of sentiment.\r\n\r\n![Boxplot of Sentiment Density Scores by Topic](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/boxplot.png)\r\n\r\n##Average Sentiment Density Scores by Topic\r\nWe next compared the average sentiment density scores by topic. The nonsense posts provide a baseline sentiment score against which we can compare the other topics. Across all posts, scores for bank services average negative and scores for customer service and public relations average positive. The takeaway is that customers have generally negative interactions with banking services, but banks appear to be able to compensate for these issues with generally positive interactions with public relations and customer service. Banks should focus on improving their Bank Services as listed above.\r\n\r\n![Bar Graph of Average Sentiment Density Scores by Topic](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/Average%20sentiment%20density%20score.png)\r\n\r\n##Average Sentiment Density Scores by Topic and Bank\r\nWe finally separated posts by bank and computed the average sentiment density scores by topic and bank. Our graph shows that sentiment scores varied in magnitude and direction by bank for each topic. For example, Bank B had the most positive sentiments for customer service and public relations, but the most negative sentiment for bank services. It is possible that Bank B is forced to compensate for poor banking services with more customer service and public relations. This information is useful for individual banks to see how their services compare to their competitors. \r\n\r\n![Boxplot of Average Sentiment Density Score by Topic and Bank](https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/topic%20and%20bank.png)\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}