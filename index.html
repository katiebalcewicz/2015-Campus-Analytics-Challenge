<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Wells Fargo 2015 Campus Analytics Challenge by katiebalcewicz</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Wells Fargo 2015 Campus Analytics Challenge</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge" class="btn">View on GitHub</a>
      <a href="https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>For the 2015 Wells Fargo Campus Analytics Challenge, students were asked to analyze a set of 220,377 anonymized social media posts to gain new insights into how customers discuss banking topics. More information about the challenge can be found on <a href="https://www.mindsumo.com/contests/wells-fargo">MindSumo</a>. For our solution, we used the R programming language to explore the data, classify the posts into categories, and analyze the sentiment of each post. The code and the other materials used in the solution can be found at <a href="https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge">github.com/katiebalcewicz/2015-Campus-Analytics-Challenge</a>.  </p>

<h1>
<a id="structuring-the-data" class="anchor" href="#structuring-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Structuring the Data</h1>

<p>The raw data that we were given was in a pipe-delimited text file that was easily read into a data frame. </p>

<pre><code>#Use the setwd() command to set the working directory to the folder that contains the dataset
setwd("~/R/Wells Fargo Competition")
#Create a Data Frame from the text file
df = read.table('dataset.txt',sep="|",header=T)
</code></pre>

<p>The first five columns were clean and easy to analyze. The sixth column held the anonymized text of the social media post, which was very messy.</p>

<p><img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/initial%20data%20frame.png" alt="Initial Data Frame"></p>

<p>In order to convert the data into a usable, structured format, we added columns to the data frame containing cleaned text, bank mentions, and sentiment scores.</p>

<h2>
<a id="cleaning-the-text" class="anchor" href="#cleaning-the-text" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning the Text</h2>

<p>The first step in structuring the data was cleaning the raw text so that it could be more easily analyzed.</p>

<h3>
<a id="removing-non-ascii-characters" class="anchor" href="#removing-non-ascii-characters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Removing non-ASCII Characters</h3>

<p>Some functions in the R packages we used did not like non-ASCII characters. The first step was to remove them from <code>FullText</code>. We defined a function called <code>removeOffendingCharacters</code> that scans through each post and replaces non-ASCII characters with "". Calling this function removes all non-ASCII characters from the <code>FullText</code> column.</p>

<pre><code>removeOffendingCharacters = function(df)
{
  df.texts.clean = as.data.frame(iconv(df$FullText, "latin1", "ASCII", sub=""))
  colnames(df.texts.clean) = 'FullText'
  df$FullText = df.texts.clean$FullText
  return(df)
}
</code></pre>

<h3>
<a id="cleaning-the-text-1" class="anchor" href="#cleaning-the-text-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning the Text</h3>

<p>We made several other modifications to the text so that it was easier for us to analyze. These include:</p>

<ul>
<li>Stripping extra whitespace</li>
<li>Converting all letters to lowercase</li>
<li>Removing stopwords (filler words that do not contribute to the essential meaning of the sentence, such as "she", "this", and "with".
*Removing numbers
*Removing all punctuation except '_'</li>
</ul>

<p>We again defined a function that makes these modifications. This function converts the <code>FullText</code> column into a corpus and applies several of the functions from the tm package to clean the text before converting it back into a data frame. This is then added to the main data frame in a new column called <code>FullTextClean</code>.</p>

<pre><code>cleanText = function(df)
{
  require(tm)
  docs = Corpus(VectorSource(df$FullText))
  docs = tm_map(docs, stripWhitespace)
  docs = tm_map(docs, tolower) 
  docs = tm_map(docs, removeWords, stopwords("english"))  
  docs = tm_map(docs, removeNumbers)   
  docs = tm_map(docs, PlainTextDocument)
  removeSomePunct = function(x) gsub("[^[:alnum:][:blank:]_]", "", x)
  docs = tm_map(docs, content_transformer(removeSomePunct))
  docs = tm_map(docs, stripWhitespace)  
  docs = tm_map(docs, PlainTextDocument)

  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, "content")), stringsAsFactors=F)
  colnames(df.texts.cleaner) = 'FullText'
  df$FullTextClean = df.texts.cleaner$FullText
  return(df)
}
</code></pre>

<h3>
<a id="stemming-the-words" class="anchor" href="#stemming-the-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stemming the words</h3>

<p>"Stemming" is a process by which all words that have the same stem (such as "analyze", "analyzed", and "analyzing") are all converted to the same stem ("analyz"). This allows us to treat each of these words as the same when we analyze the data. We defined a function that applies this process to the <code>FullTextClean</code> column and adds the results to the main data frame in a new column called <code>FullTextStemmed</code>.</p>

<pre><code>stemText = function(df)
{
  require(tm)
  require(SnowballC)
  docs = Corpus(VectorSource(df$FullTextClean))
  docs = tm_map(docs, stemDocument)
  docs = tm_map(docs, PlainTextDocument)
  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, "content")), stringsAsFactors=F)
  colnames(df.texts.cleaner) = 'FullText'
  df$FullTextStemmed = df.texts.cleaner$FullText
  return(df)
}
</code></pre>

<h3>
<a id="stem-completion" class="anchor" href="#stem-completion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stem Completion</h3>

<p>Stem Completion converts the stems from the previous step ("analyz") back into a dictionary word ("analyze"). Unfortunately, this algorithm was too computationally intensive to apply to the full dataset. We designed this function to use multithreading in the hopes that we might get the chance to run the code on a computer with more cores. If you were to run this function, it would add another column to the main data frame called <code>FullTextCompleted</code>.</p>

<pre><code>stemCompleteText = function(df)
{
  require(plyr)
  require(doMC)
  require(tm)
  doMC::registerDoMC(cores=3) #Change the number of threads here to match your machine.
  docs = Corpus(VectorSource(df$FullTextStemmed))
  dictionary = Corpus(VectorSource(df$FullText))

  stemCompletion2 &lt;- function(y, dictionary) 
  {
    y = unlist(strsplit(as.character(y), " "))
    y = y[y != ""]
    y = stemCompletion(y, dictionary=dictionary)
    y = paste(y, sep="", collapse=" ")
    stripWhitespace(y)
  }

  completedText = llply(docs, stemCompletion2, dictionary=dictionary, .parallel = TRUE)
  df$FullTextCompleted = unlist(completedText)
  return(df)
}
</code></pre>

<h2>
<a id="analyzing-bank-mentions" class="anchor" href="#analyzing-bank-mentions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Analyzing bank mentions</h2>

<p>The next step to structuring the data was separating it by bank. We identified which bank was mentioned in each post. We also counted the number of banks mentioned in each post. This function adds six new columns to the data frame. The first 5, for banks A-E, hold a 1 if the bank is mentioned and a 0 if it is not. The sixth is a sum of the previous five columns and is equivalent to the number of banks mentioned in the post. Most posts mention one bank.</p>

<pre><code>bankMentions = function(df)
{
  df$BankA = 0
  df$BankB = 0
  df$BankC = 0
  df$BankD = 0
  df$BankE = 0
  df$NumBanks = 0
  for( i in 1:nrow(df))
  {
    if (grepl("BankA",df$FullText[i]))  df$BankA[i] = 1
    if (grepl("BankB",df$FullText[i]))  df$BankB[i] = 1
    if (grepl("BankC",df$FullText[i]))  df$BankC[i] = 1
    if (grepl("BankD",df$FullText[i]))  df$BankD[i] = 1
    if (grepl("banke",df$FullText[i]))  df$BankE[i] = 1
    df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]
  }
  return(df)
}
</code></pre>

<h2>
<a id="sentiment-analysis" class="anchor" href="#sentiment-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sentiment Analysis</h2>

<p>Our third step for structuring the data was to determine whether each post was positive or negative. We assigned each post a score based on the number of positive and negative words that it contained. We began by importing lists of positive words and negative words from text files. Then, for each post, we counted the number of positive word matches and negative words matches. The number of positive matches minus the number of negative matches equals the sentiment score. This is added to the data frame in a column called <code>SentimentScore</code>. However, we found that this was skewed towards extremes for posts with more words (i.e. facebook posts). In order to normalize the sentiment score, we created an additional column called <code>SentimentDensityScore</code> that was calculated by dividing the sentiment score by the number of words in the post. We also added two columns called <code>very.pos</code> and <code>very.neg</code> which contain 1 if the sentiment score is greater than 2 or less than -2, respectively, 0 otherwise. The function below applies the above procedure using the <code>plyr</code> package.</p>

<pre><code>sentimentAnalysis = function(df)
{
  pos &lt;- scan('positive-words.txt',what='character',comment.char=';')
  neg &lt;- scan('negative-words.txt',what='character',comment.char=';')
  require(plyr)
  require(stringr)

  scores = laply(df$FullTextClean, function(sentence, pos.words, neg.words) {
    sentence = as.character(sentence)
    word.list = str_split(sentence, '\\s+')
    words = unlist(word.list)
    words.length = length(words)

    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)

    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)

    score = sum(pos.matches) - sum(neg.matches)
    if(words.length == 0) score.density = 0
    else score.density = score/words.length
    score.list = c(score, score.density)

    return(score.list)
  }, pos, neg, .progress = 'text')

  scores.df = data.frame(SentimentScore=scores[,1], SentimentDensity=scores[,2])
  df$SentimentScore = scores[,1]
  df$SentimentDensity = scores[,2]
  df$very.pos = as.numeric(df$SentimentScore &gt;= 2)
  df$very.neg = as.numeric(df$SentimentDensity &lt;= -2)
  return(df)
}
</code></pre>

<h1>
<a id="exploratory-data-analysis" class="anchor" href="#exploratory-data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploratory Data Analysis</h1>

<p>Once the data was cleaned up, we were able to begin our initial exploratory analysis. We began by creating a Document Term Matrix, which contained 95109 terms. Removing the terms with greater than 99.9% sparsity left us with a much more manageable matrix of only 1323 terms. Since the matrix was so large (220377 rows), we converted it into a Simple Triplet Matrix so that we could perform computations on it.</p>

<pre><code>docs = Corpus(VectorSource(df$FullTextStemmed))
dtm = DocumentTermMatrix(docs)   
dtm.sparse = dtms &lt;- removeSparseTerms(dtm, 0.999)
dtm.sparse.simple = as.simple_triplet_matrix(dtm.sparse) 
</code></pre>

<p>We computed the word frequencies by summing the entries of each column and sorting the list from most to least occurrences. We removed the first nine most frequent words, which included the name of each bank and the words that were used by the competition organizers to anonymize the posts ("name", "internet", "twit_hndl", and "name_resp"), because they occurred much more often than any other words.</p>

<pre><code>freq &lt;- sort(col_sums(dtm.sparse.simple), decreasing=TRUE)   
freq = freq[-c(1:9)]
wf &lt;- data.frame(word=names(freq), freq=freq)  
</code></pre>

<h2>
<a id="plot-a-bar-graph" class="anchor" href="#plot-a-bar-graph" aria-hidden="true"><span class="octicon octicon-link"></span></a>Plot a Bar Graph</h2>

<p>We created a bar graph of the words occurring more than 6000 times.
<img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/bar%20chart.png" alt="Bar Graph of Frequent Words"></p>

<h2>
<a id="create-a-word-cloud" class="anchor" href="#create-a-word-cloud" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create a Word Cloud</h2>

<p>We created a word cloud of the 60 most frequent words.
<img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/word%20cloud%2002.png" alt="Word Cloud of frequent words"></p>

<h2>
<a id="find-words-associated-with-each-bank" class="anchor" href="#find-words-associated-with-each-bank" aria-hidden="true"><span class="octicon octicon-link"></span></a>Find Words Associated with Each Bank</h2>

<p>Using the <code>findAssocs</code> function, we identified words that are highly correlated with the mention of each bank name. Notice that BankA and BankB are both frequently mentioned in association with buildings and sporting events that they sponsor. This easily allows us to identify the name of the banks. For example, try googling "center arena Philadelphia". 
<img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/assocs%202.png" alt="Words highly associated with each bank"></p>

<h1>
<a id="classify-the-posts" class="anchor" href="#classify-the-posts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classify the Posts</h1>

<p>Based on the frequent words found in our exploratory data analysis, we manually identified four topics: Customer Service, Bank Services, Public Relations, and Nonsense. We pulled a random subset of 600 posts and manually labeled each one with the topic we identified. The following code demonstrates how we developed a Random Forest Classifier that we used to label the rest of the posts in the dataset.</p>

<h2>
<a id="import-the-training-dataset" class="anchor" href="#import-the-training-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Import the training dataset</h2>

<p>We imported the training data set that was manually labeled by topic. We applied the same cleaning and structuring that we used for the main dataset. We then created a non-sparse Document Term Matrix to identify which posts held which words. We converted this back to a data frame so that we could use the classifier on it. We added three columns to the data frame that were also used in the classification: MediaType, very.pos, and very.neg. Finally we added the topic so that the classifier could be trained to identify it.</p>

<pre><code>train = read.csv("train.csv")
train = createDataFrame(train)
docs.train = Corpus(VectorSource(train$FullTextStemmed))
dtm.train = DocumentTermMatrix(docs.train) 
dtm.sparse.train = removeSparseTerms(dtm.train, 0.99)
terms = as.data.frame(as.matrix(dtm.sparse.train))
terms$MediaType = train$MediaType
terms$very.pos = train$very.pos
terms$very.neg = train$very.neg
terms$Topic = train$Topic
</code></pre>

<h2>
<a id="split-the-training-dataset-for-cross-validation" class="anchor" href="#split-the-training-dataset-for-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Split the training dataset for cross validation</h2>

<p>We used the holdout method of cross-validation to estimate the accuracy of our classifier. Our holdout set accounted for about 1/3 of the data. Using a Random Forest classifier, the accuracy comes out to about 78%. We also tested a CART decision tree created using rpart and a Naive Bayes classifier, but both had worse accuracy.</p>

<pre><code>terms.train = droplevels(terms[c(1:399),])
terms.test = droplevels(terms[c(400:nrow(terms)),])
classifier = randomForest(Topic ~ ., data = terms.train)
prediction = predict(classifier, terms.test)
terms.test$Prediction = prediction
terms.test$Accuracy = as.numeric(terms.test$Topic == terms.test$Prediction)
accuracy = sum(terms.test$Accuracy)/nrow(terms.test)
</code></pre>

<h2>
<a id="build-the-random-forest-model" class="anchor" href="#build-the-random-forest-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Build the Random Forest Model</h2>

<p>A random forest classifier is essentially a set of many decision trees whose results are averaged to decide the classification. In this case, our random forest has 500 trees.</p>

<pre><code>classifier = randomForest(Topic ~ ., data = terms)
</code></pre>

<p>Although the classifier cannot be easily visualized, since it has so many trees, we can view which fields were important in the classification.</p>

<p><img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/classifier%20importance%20plot.png" alt="Important fields for classification"></p>

<h2>
<a id="create-the-test-data-frame-for-the-classifier" class="anchor" href="#create-the-test-data-frame-for-the-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the test data frame for the classifier</h2>

<p>To create the testing data frame, we used the same approach as for creating the training data frame. We created a non-sparse Document Term Matrix, converted back to a data frame, and added the additional fields of MediaType, very.pos, and very.neg.</p>

<pre><code>test.terms = as.data.frame(as.matrix(dtm.sparse))
test.terms$MediaType = df$MediaType
test.terms$very.pos = df$very.pos
test.terms$very.neg = df$very.neg
</code></pre>

<h2>
<a id="apply-the-classifier-to-the-test-data-frame" class="anchor" href="#apply-the-classifier-to-the-test-data-frame" aria-hidden="true"><span class="octicon octicon-link"></span></a>Apply the classifier to the test data frame</h2>

<p>We applied the classifier to the test data frame and put the results in the main data frame in a column called <code>Topic</code>.</p>

<pre><code>prediction = predict(classifier, test.terms)
df$Topic = prediction
</code></pre>

<h1>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h1>

<p>The final data frame has 13 columns in addition to the original 6. Based on these columns, we were able to create several graphs from which we drew conclusions about the data.</p>

<h2>
<a id="boxplot-of-sentiment-density-scores-by-topic" class="anchor" href="#boxplot-of-sentiment-density-scores-by-topic" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boxplot of Sentiment Density Scores By Topic</h2>

<p>We created a boxplot that plotted Sentiment Density Scores by topic. From this you can see that nonsense posts made up the majority of all posts and have the greatest spread of sentiment.</p>

<p><img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/boxplot.png" alt="Boxplot of Sentiment Density Scores by Topic"></p>

<h2>
<a id="average-sentiment-density-scores-by-topic" class="anchor" href="#average-sentiment-density-scores-by-topic" aria-hidden="true"><span class="octicon octicon-link"></span></a>Average Sentiment Density Scores by Topic</h2>

<p>We next compared the average sentiment density scores by topic. The nonsense posts provide a baseline sentiment score against which we can compare the other topics. Across all posts, scores for bank services average negative and scores for customer service and public relations average positive. The takeaway is that customers have generally negative interactions with banking services, but banks appear to be able to compensate for these issues with generally positive interactions with public relations and customer service. Banks should focus on improving their Bank Services as listed above.</p>

<p><img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/Average%20sentiment%20density%20score.png" alt="Bar Graph of Average Sentiment Density Scores by Topic"></p>

<h2>
<a id="average-sentiment-density-scores-by-topic-and-bank" class="anchor" href="#average-sentiment-density-scores-by-topic-and-bank" aria-hidden="true"><span class="octicon octicon-link"></span></a>Average Sentiment Density Scores by Topic and Bank</h2>

<p>We finally separated posts by bank and computed the average sentiment density scores by topic and bank. Our graph shows that sentiment scores varied in magnitude and direction by bank for each topic. For example, Bank B had the most positive sentiments for customer service and public relations, but the most negative sentiment for bank services. It is possible that Bank B is forced to compensate for poor banking services with more customer service and public relations. This information is useful for individual banks to see how their services compare to their competitors. </p>

<p><img src="https://raw.githubusercontent.com/katiebalcewicz/2015-Campus-Analytics-Challenge/images/Average%20score%20by%20topic%20and%20bank.png" alt="Boxplot of Average Sentiment Density Score by Topic and Bank"></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/katiebalcewicz/2015-Campus-Analytics-Challenge">Wells Fargo 2015 Campus Analytics Challenge</a> is maintained by <a href="https://github.com/katiebalcewicz">katiebalcewicz</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
